@inproceedings{NACCLacronym,
	author =       {Hu, Hai},
	title =        {Is {China} entering {WTO} or shijie maoyi zuzhi--A Corpus-based Study of {English} Acronyms in {Chinese} Newspapers},
	booktitle =    "Proceedings of 28th North America Conference on Chinese Linguistics",
	year =         "2016",
	url={https://arxiv.org/abs/1711.06895},
	
}

@inproceedings{NACCL2017chengduvowel,
	author =       {Hu, Hai and Zhang, Yiwen},
	title =        {Path of Vowel Raising in {Chengdu} Dialect of {Mandarin}},
	booktitle =    "Proceedings of 29th North America Conference on Chinese Linguistics",
	year =         "2017",
	pages={481--498},
	url = {https://arxiv.org/pdf/1803.03887.pdf},
}

@inproceedings{LFG,
	author =       {Damir Cavar and Lwin Moe and Hai Hu and Kenneth Steimel},
	title =        {Preliminary Results from the {Free Linguistic Environment} Project},
	booktitle =    "Joint 2016 Conference on Head-driven Phrase Structure Grammar and Lexical Functional Grammar (HeadLex 2016)",
	year =         "2016",
	pages={161--181},
	url = { http://nlp.ipipan.waw.pl/HeadLex16/cavar_etal.pdf},
}

@inproceedings{hu-etal-2017-non,
    title = "Non-Deterministic Segmentation for {C}hinese Lattice Parsing",
    author = {Hu, Hai  and
      Dakota, Daniel  and
      K{\"u}bler, Sandra},
    booktitle = "Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",
    month = sep,
    year = "2017",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://doi.org/10.26615/978-954-452-049-6_043",
    doi = "10.26615/978-954-452-049-6_043",
    pages = "316--324",
    abstract = "Parsing Chinese critically depends on correct word segmentation for the parser since incorrect segmentation inevitably causes incorrect parses. We investigate a pipeline approach to segmentation and parsing using word lattices as parser input. We compare CRF-based and lexicon-based approaches to word segmentation. Our results show that the lattice parser is capable of selecting the correction segmentation from thousands of options, thus drastically reducing the number of unparsed sentence. Lexicon-based parsing models have a better coverage than the CRF-based approach, but the many options are more difficult to handle. We reach our best result by using a lexicon from the n-best CRF analyses, combined with highly probable words.",
}

@inproceedings{NLCS2018,
	author={Hu, Hai and Icard, Thomas and Moss, Larry},
	title={Automated Reasoning from Polarized Parse Trees},
	booktitle={Proceedings of the Fifth Workshop on Natural Language and Computer Science (NLCS), 2018},
	year={2018},
	url={https://easychair.org/publications/preprint/xJmn}
}

@inproceedings{hu-moss-2018-polarity,
    title = "Polarity Computations in Flexible Categorial Grammar",
    author = "Hu, Hai  and
      Moss, Larry",
    booktitle = "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S18-2015",
    doi = "10.18653/v1/S18-2015",
    pages = "124--129",
    abstract = "This paper shows how to take parse trees in CCG and algorithmically find the polarities of all the constituents. Our work uses the well-known polarization principle corresponding to function application, and we have extended this with principles for type raising and composition. We provide an algorithm, extending the polarity marking algorithm of van Benthem. We discuss how our system works in practice, taking input from the C{\&}C parser.",
}


@inproceedings{hu-etal-2018-detecting,
    title = "Detecting Syntactic Features of Translated {C}hinese",
    author = {Hu, Hai  and
      Li, Wen  and
      K{\"u}bler, Sandra},
    booktitle = "Proceedings of the Second Workshop on Stylistic Variation",
    month = jun,
    year = "2018",
    address = "New Orleans",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-1603",
    doi = "10.18653/v1/W18-1603",
    pages = "20--28",
    abstract = "We present a machine learning approach to distinguish texts translated to Chinese (by humans) from texts originally written in Chinese, with a focus on a wide range of syntactic features. Using Support Vector Machines (SVMs) as classifier on a genre-balanced corpus in translation studies of Chinese, we find that constituent parse trees and dependency triples as features without lexical information perform very well on the task, with an F-measure above 90{\%}, close to the results of lexical n-gram features, without the risk of learning topic information rather than translation features. Thus, we claim syntactic features alone can accurately distinguish translated from original Chinese. Translated Chinese exhibits an increased use of determiners, subject position pronouns, NP + {``}çš„{''} as NP modifiers, multiple NPs or VPs conjoined by ''ã€'', among other structures. We also interpret the syntactic features with reference to previous translation studies in Chinese, particularly the usage of pronouns.",
}


@inproceedings{hu-etal-2019-natural,
    title = "Natural Language Inference with Monotonicity",
    author = "Hu, Hai  and
      Chen, Qi  and
      Moss, Larry",
    booktitle = "Proceedings of the 13th International Conference on Computational Semantics - Short Papers",
    month = may,
    year = "2019",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-0502",
    doi = "10.18653/v1/W19-0502",
    pages = "8--15",
    abstract = "This paper describes a working system which performs natural language inference using polarity-marked parse trees. The system handles all of the instances of monotonicity inference in the FraCaS data set. Except for the initial parse, it is entirely deterministic. It handles multi-premise arguments, and the kind of inference performed is essentially {``}logical{''}, but it goes beyond what is representable in first-order logic. In any case, the system works on surface forms rather than on representations of any kind.",
}

@inproceedings{hu-etal-2019-ensemble,
    title = "Ensemble Methods to Distinguish Mainland and {T}aiwan {C}hinese",
    author = "Hu, Hai  and
      Li, Wen  and
      Zhou, He  and
      Tian, Zuoyu  and
      Zhang, Yiwen  and
      Zou, Liang",
    booktitle = "Proceedings of the Sixth Workshop on {NLP} for Similar Languages, Varieties and Dialects",
    month = jun,
    year = "2019",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1417",
    doi = "10.18653/v1/W19-1417",
    pages = "165--171",
    abstract = "This paper describes the IUCL system at VarDial 2019 evaluation campaign for the task of discriminating between Mainland and Taiwan variation of mandarin Chinese. We first build several base classifiers, including a Naive Bayes classifier with word n-gram as features, SVMs with both character and syntactic features, and neural networks with pre-trained character/word embeddings. Then we adopt ensemble methods to combine output from base classifiers to make final predictions. Our ensemble models achieve the highest F1 score (0.893) in simplified Chinese track and the second highest (0.901) in traditional Chinese track. Our results demonstrate the effectiveness and robustness of the ensemble methods.",
}

@incollection{CKIP2018,
	author      = {Lin, Chien-Jer Charles and Hu, Hai},
	title       = {Linking Comprehension and Production: Frequency Distribution of {Chinese} Relative Clauses in {Sinica} Treebank},
	editor      = {Huang, Chu-Ren and Hsieh, Shu-kai and Sui, Zhifang},
	booktitle   = {Chinese Language Resources: Data Collection, Linguistic Analysis, Annotation, and Language Processing},
	publisher   = {Springer},
	year        = {2018},
}

@article{algomethod,
  title={Word Embeddings and Semantic Shifts in Historical Spanish: Methodological Considerations},
  author={Hai Hu and Patr\'{i}cia Amaral and Sandra K\"{u}bler},
  journal={Digital Scholarship in the Humanities},
  year={accepted},
}

@article{NLE2020,
	author = {Hu, Hai and K{\"u}bler, Sandra},
	title = {Investigating Translated Chinese and Its Variants Using Machine Learning},
	journal = {Journal of Natural Language Engineering (Special Issue on NLP for Similar Languages, Varieties and Dialects)},
	year = {2020},
	pages={1--34},
	publisher={Cambridge University Press},
	DOI={10.1017/S1351324920000182},
	url={https://www.cambridge.org/core/journals/natural-language-engineering/article/investigating-translated-chinese-and-its-variants-using-machine-learning/E640C7C90BB48C36694AD0A01E2A5049}
}


@inproceedings{hu-etal-2020-monalog,
    title = "{M}ona{L}og: a Lightweight System for Natural Language Inference Based on Monotonicity",
    author = "Hu, Hai  and
      Chen, Qi  and
      Richardson, Kyle  and
      Mukherjee, Atreyee  and
      Moss, Lawrence S.  and
      Kuebler, Sandra",
    booktitle = "Proceedings of the Society for Computation in Linguistics 2020",
    month = jan,
    year = "2020",
    address = "New York, New York",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.scil-1.40",
    pages = "334--344",
}

@inproceedings{aaai2020,
  title={Probing Natural Language Inference Models through Semantic Fragments},
  author={Richardson, Kyle and Hai Hu and Lawrence S Moss and Ashish Sabharwal},
  booktitle={Proceedings of The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)},
  year={2020},
  pages={8713--8721},
  url={https://arxiv.org/abs/1909.07521}
}

@inproceedings{xu-etal-2020-clue,
    title = "{CLUE}: A {C}hinese Language Understanding Evaluation Benchmark",
    author = "Xu, Liang  and
      Hu, Hai  and
      Zhang, Xuanwei  and
      Li, Lu  and
      Cao, Chenjie  and
      Li, Yudong  and
      Xu, Yechen  and
      Sun, Kai  and
      Yu, Dian  and
      Yu, Cong  and
      Tian, Yin  and
      Dong, Qianqian  and
      Liu, Weitang  and
      Shi, Bo  and
      Cui, Yiming  and
      Li, Junyi  and
      Zeng, Jun  and
      Wang, Rongzhao  and
      Xie, Weijian  and
      Li, Yanting  and
      Patterson, Yina  and
      Tian, Zuoyu  and
      Zhang, Yiwen  and
      Zhou, He  and
      Liu, Shaoweihua  and
      Zhao, Zhe  and
      Zhao, Qipeng  and
      Yue, Cong  and
      Zhang, Xinrui  and
      Yang, Zhengliang  and
      Richardson, Kyle  and
      Lan, Zhenzhong",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.coling-main.419",
    doi = "10.18653/v1/2020.coling-main.419",
    pages = "4762--4772",
    abstract = "The advent of natural language understanding (NLU) benchmarks for English, such as GLUE and SuperGLUE allows new NLU models to be evaluated across a diverse set of tasks. These comprehensive benchmarks have facilitated a broad range of research and applications in natural language processing (NLP). The problem, however, is that most such benchmarks are limited to English, which has made it difficult to replicate many of the successes in English NLU for other languages. To help remedy this issue, we introduce the first large-scale Chinese Language Understanding Evaluation (CLUE) benchmark. CLUE is an open-ended, community-driven project that brings together 9 tasks spanning several well-established single-sentence/sentence-pair classification tasks, as well as machine reading comprehension, all on original Chinese text. To establish results on these tasks, we report scores using an exhaustive set of current state-of-the-art pre-trained Chinese models (9 in total). We also introduce a number of supplementary datasets and additional tools to help facilitate further progress on Chinese NLU. Our benchmark is released at https://www.cluebenchmarks.com",
}


@InProceedings{lightmodel2020,
author="Li, Junyi
and Hu, Hai
and Zhang, Xuanwei
and Li, Minglei
and Li, Lu
and Xu, Liang",
editor="Zhu, Xiaodan
and Zhang, Min
and Hong, Yu
and He, Ruifang",
title="Light Pre-Trained Chinese Language Model for NLP Tasks",
booktitle="Natural Language Processing and Chinese Computing",
year="2020",
publisher="Springer International Publishing",
pages="567--578",
abstract="We present the results of shared-task 1 held in the 2020 Conference on Natural Language Processing and Chinese Computing (NLPCC): Light Pre-Trained Chinese Language Model for NLP tasks. This shared-task examines the performance of light language models on four common NLP tasks: Text Classification, Named Entity Recognition, Anaphora Resolution and Machine Reading Comprehension. To make sure that the models are light-weight, we put restrictions and requirements on the number of parameters and inference speed of the participating models. In total, 30 teams registered our tasks. Each submission was evaluated through our online benchmark system (https://www.cluebenchmarks.com/nlpcc2020.html), with the average score over the four tasks as the final score. Various ideas and frameworks were explored by the participants, including data enhancement, knowledge distillation and quantization. The best model achieved an average score of 75.949, which was very close to BERT-base (76.460). We believe this shared-task highlights the potential of light-weight models and calls for further research on the development and exploration of light-weight models.",
isbn="978-3-030-60457-8",
url={https://rdcu.be/b8mRQ}
}

@inproceedings{hu-etal-2020-ocnli,
    title = "{OCNLI}: {O}riginal {C}hinese {N}atural {L}anguage {I}nference",
    author = {Hu, Hai  and
      Richardson, Kyle  and
      Xu, Liang  and
      Li, Lu  and
      K{\"u}bler, Sandra  and
      Moss, Lawrence},
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.314",
    doi = "10.18653/v1/2020.findings-emnlp.314",
    pages = "3512--3526",
    abstract = "Despite the tremendous recent progress on natural language inference (NLI), driven largely by large-scale investment in new datasets (e.g.,SNLI, MNLI) and advances in modeling, most progress has been limited to English due to a lack of reliable datasets for most of the world{'}s languages. In this paper, we present the first large-scale NLI dataset (consisting of {\textasciitilde}56,000 annotated sentence pairs) for Chinese called the Original Chinese Natural Language Inference dataset (OCNLI). Unlike recent attempts at extending NLI to other languages, our dataset does not rely on any automatic translation or non-expert annotation. Instead, we elicit annotations from native speakers specializing in linguistics. We follow closely the annotation protocol used for MNLI, but create new strategies for eliciting diverse hypotheses. We establish several baseline results on our dataset using state-of-the-art pre-trained models for Chinese, and find even the best performing models to be far outpaced by human performance ({\textasciitilde}12{\%} absolute performance gap), making it a challenging new resource that we hope will help to accelerate progress in Chinese NLU. To the best of our knowledge, this is the first human-elicited MNLI-style corpus for a non-English language.",
}

@inproceedings{hu-etal-2020-building,
    title = "Building a Treebank for {C}hinese Literature for Translation Studies",
    author = "Hu, Hai  and
      Li, Yanting  and
      Patterson, Yina  and
      Tian, Zuoyu  and
      Zhang, Yiwen  and
      Zhou, He  and
      Kuebler, Sandra  and
      Lin, Chien-Jer Charles",
    booktitle = "Proceedings of the 19th International Workshop on Treebanks and Linguistic Theories",
    month = oct,
    year = "2020",
    address = {D{\"u}sseldorf, Germany},
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.tlt-1.2",
    doi = "10.18653/v1/2020.tlt-1.2",
    pages = "18--30",
}


@inproceedings{diagnosticsNLI,
	title={Understanding Transfer Learning of Multi-lingual Pre-trained Language
Models through {Chinese} Natural Language Inference},
	author={Hu, Hai and Zhou, He and  Tian, Zuoyu and  Zhang, Yiwen and Patterson, Yina  and  Li, Yanting and Nie, Yixin  and  Richardson, Kyle},
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2021",
	year={2021}
}

